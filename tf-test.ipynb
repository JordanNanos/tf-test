{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure proxy setup if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['http_proxy'] = <<ENTER PROXY HERE>>\n",
    "os.environ['https_proxy'] = <<ENTER PROXY HERE>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-estimator==2.1.0\r\n",
      "tensorflow-gpu==2.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 5 12 21 32], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant([1,2,3,4])\n",
    "x2 = tf.constant([5,6,7,8])\n",
    "result = tf.multiply(x1,x2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(100, input_shape=(8,)))\n",
    "model.add(Dense(80))\n",
    "model.add(Dense(30))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "\n",
    "x_in = Input(shape=(8,))\n",
    "x = Dense(10)(x_in)\n",
    "x_out = Dense(1)(x)\n",
    "model = Model(inputs=x_in, outputs=x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-24 03:43:19--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv\n",
      "Connecting to 10.40.0.1:3130... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 76466 (75K) [text/plain]\n",
      "Saving to: ‘ionosphere.csv’\n",
      "\n",
      "ionosphere.csv      100%[===================>]  74.67K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-03-24 03:43:19 (626 KB/s) - ‘ionosphere.csv’ saved [76466/76466]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235, 34) (116, 34) (235,) (116,)\n",
      "Test Accuracy: 0.905\n",
      "Predicted: 0.996\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# load the dataset\n",
    "path = '/tf/ionosphere.csv'\n",
    "df = read_csv(path, header=None)\n",
    "# split into input and output columns\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "# ensure all data are floating point values\n",
    "X = X.astype('float32')\n",
    "# encode strings to integer\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)\n",
    "# evaluate the model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "# make a prediction\n",
    "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "yhat = model.predict([row])\n",
    "print('Predicted: %.3f' % yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-24 03:48:54--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\n",
      "Connecting to 10.40.0.1:3130... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 4549 (4.4K) [text/plain]\n",
      "Saving to: ‘iris.csv.1’\n",
      "\n",
      "iris.csv.1          100%[===================>]   4.44K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2021-03-24 03:48:55 (2.42 MB/s) - ‘iris.csv.1’ saved [4549/4549]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4) (50, 4) (100,) (50,)\n",
      "Test Accuracy: 0.960\n",
      "Predicted: [[0.96548826 0.02517315 0.00933858]] (class=0)\n"
     ]
    }
   ],
   "source": [
    "# mlp for multiclass classification\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# load the dataset\n",
    "path = '/tf/iris.csv'\n",
    "df = read_csv(path, header=None)\n",
    "# split into input and output columns\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "# ensure all data are floating point values\n",
    "X = X.astype('float32')\n",
    "# encode strings to integer\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)\n",
    "# evaluate the model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "# make a prediction\n",
    "row = [5.1,3.5,1.4,0.2]\n",
    "yhat = model.predict([row])\n",
    "print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-24 03:51:01--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv\n",
      "Connecting to 10.40.0.1:3130... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 41295 (40K) [text/plain]\n",
      "Saving to: ‘housing.csv’\n",
      "\n",
      "housing.csv         100%[===================>]  40.33K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2021-03-24 03:51:01 (468 KB/s) - ‘housing.csv’ saved [41295/41295]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (167, 13) (339,) (167,)\n",
      "MSE: 43.580, RMSE: 6.602\n",
      "Predicted: 28.959\n"
     ]
    }
   ],
   "source": [
    "from numpy import sqrt\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# load the dataset\n",
    "path = '/tf/housing.csv'\n",
    "df = read_csv(path, header=None)\n",
    "# split into input and output columns\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)\n",
    "# evaluate the model\n",
    "error = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (error, sqrt(error)))\n",
    "# make a prediction\n",
    "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
    "yhat = model.predict([row])\n",
    "print('Predicted: %.3f' % yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# broken right now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "URL fetch failure on https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz: None -- [Errno 104] Connection reset by peer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1318\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1414\u001b[0m             self.sock = self._context.wrap_socket(self.sock,\n\u001b[0;32m-> 1415\u001b[0;31m                                                   server_hostname=server_hostname)\n\u001b[0m\u001b[1;32m   1416\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_hostname\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    406\u001b[0m                          \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                          _context=self, _session=session)\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context, _session)\u001b[0m\n\u001b[1;32m    816\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1076\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;34m\"\"\"Start the SSL/TLS handshake.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 104] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 544\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1361\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 104] Connection reset by peer>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-f0243c269dbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# summarize loaded dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train: X=%s, y=%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/datasets/mnist.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morigin_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'mnist.npz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0mfile_hash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m       '731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1')\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: URL fetch failure on https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz: None -- [Errno 104] Connection reset by peer"
     ]
    }
   ],
   "source": [
    "# example of loading and plotting the mnist dataset\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "(trainX, trainy), (testX, testy) = load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
    "# plot first few images\n",
    "for i in range(25):\n",
    "\t# define subplot\n",
    "\tpyplot.subplot(5, 5, i+1)\n",
    "\t# plot raw pixel data\n",
    "\tpyplot.imshow(trainX[i], cmap=pyplot.get_cmap('gray'))\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-24 03:55:37--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-car-sales.csv\n",
      "Connecting to 10.40.0.1:3130... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 1834 (1.8K) [text/plain]\n",
      "Saving to: ‘monthly-car-sales.csv’\n",
      "\n",
      "monthly-car-sales.c 100%[===================>]   1.79K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-03-24 03:55:38 (60.6 MB/s) - ‘monthly-car-sales.csv’ saved [1834/1834]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-car-sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 5, 1) (12, 5, 1) (91,) (12,)\n",
      "Train on 91 samples, validate on 12 samples\n",
      "Epoch 1/350\n",
      "91/91 - 1s - loss: 35721365.0110 - mae: 4800.4409 - val_loss: 28484714.0000 - val_mae: 4448.6309\n",
      "Epoch 2/350\n",
      "91/91 - 0s - loss: 22176512.8132 - mae: 3822.5750 - val_loss: 21696788.0000 - val_mae: 3982.7598\n",
      "Epoch 3/350\n",
      "91/91 - 0s - loss: 20524341.6923 - mae: 3692.5015 - val_loss: 24611040.0000 - val_mae: 4437.2612\n",
      "Epoch 4/350\n",
      "91/91 - 0s - loss: 21991428.2198 - mae: 3812.9375 - val_loss: 22218148.0000 - val_mae: 3816.7126\n",
      "Epoch 5/350\n",
      "91/91 - 0s - loss: 19530273.2967 - mae: 3660.5061 - val_loss: 15099708.0000 - val_mae: 3351.8660\n",
      "Epoch 6/350\n",
      "91/91 - 0s - loss: 18455456.8352 - mae: 3418.2661 - val_loss: 13770700.0000 - val_mae: 3301.3665\n",
      "Epoch 7/350\n",
      "91/91 - 0s - loss: 18168215.7582 - mae: 3441.3623 - val_loss: 18065902.0000 - val_mae: 3549.0977\n",
      "Epoch 8/350\n",
      "91/91 - 0s - loss: 16443787.2747 - mae: 3300.7273 - val_loss: 12232637.0000 - val_mae: 2660.4792\n",
      "Epoch 9/350\n",
      "91/91 - 0s - loss: 14961384.8681 - mae: 3162.8701 - val_loss: 13175504.0000 - val_mae: 2961.2756\n",
      "Epoch 10/350\n",
      "91/91 - 0s - loss: 12815800.0934 - mae: 2998.7451 - val_loss: 12515748.0000 - val_mae: 2859.9866\n",
      "Epoch 11/350\n",
      "91/91 - 0s - loss: 12623260.9011 - mae: 2988.2041 - val_loss: 11653875.0000 - val_mae: 2865.8884\n",
      "Epoch 12/350\n",
      "91/91 - 0s - loss: 12558944.1319 - mae: 3051.7026 - val_loss: 18708296.0000 - val_mae: 3371.1875\n",
      "Epoch 13/350\n",
      "91/91 - 0s - loss: 14567618.8681 - mae: 3283.2444 - val_loss: 17611694.0000 - val_mae: 3443.4060\n",
      "Epoch 14/350\n",
      "91/91 - 0s - loss: 16030662.2637 - mae: 3331.0808 - val_loss: 16539589.0000 - val_mae: 3332.5383\n",
      "Epoch 15/350\n",
      "91/91 - 0s - loss: 13087834.4835 - mae: 3069.0076 - val_loss: 16781724.0000 - val_mae: 3306.3284\n",
      "Epoch 16/350\n",
      "91/91 - 0s - loss: 11932892.0604 - mae: 2946.1213 - val_loss: 15237757.0000 - val_mae: 3336.3074\n",
      "Epoch 17/350\n",
      "91/91 - 0s - loss: 11119616.2088 - mae: 2837.8987 - val_loss: 13766181.0000 - val_mae: 3041.2454\n",
      "Epoch 18/350\n",
      "91/91 - 0s - loss: 10461286.0440 - mae: 2770.2629 - val_loss: 18931466.0000 - val_mae: 3354.9641\n",
      "Epoch 19/350\n",
      "91/91 - 0s - loss: 10630891.9231 - mae: 2659.7771 - val_loss: 19207162.0000 - val_mae: 3387.8477\n",
      "Epoch 20/350\n",
      "91/91 - 0s - loss: 10478651.5714 - mae: 2709.6750 - val_loss: 17823230.0000 - val_mae: 3180.9402\n",
      "Epoch 21/350\n",
      "91/91 - 0s - loss: 11929228.5824 - mae: 2783.4248 - val_loss: 14559909.0000 - val_mae: 3231.8772\n",
      "Epoch 22/350\n",
      "91/91 - 0s - loss: 12247326.3077 - mae: 2822.4375 - val_loss: 15726255.0000 - val_mae: 3137.7820\n",
      "Epoch 23/350\n",
      "91/91 - 0s - loss: 17077421.9121 - mae: 3151.8169 - val_loss: 15848779.0000 - val_mae: 3138.3633\n",
      "Epoch 24/350\n",
      "91/91 - 0s - loss: 12153338.7582 - mae: 2838.4597 - val_loss: 15669273.0000 - val_mae: 3250.9929\n",
      "Epoch 25/350\n",
      "91/91 - 0s - loss: 10527818.8132 - mae: 2641.6917 - val_loss: 21950776.0000 - val_mae: 3791.1926\n",
      "Epoch 26/350\n",
      "91/91 - 0s - loss: 11013870.0000 - mae: 2696.7920 - val_loss: 13810679.0000 - val_mae: 3244.6707\n",
      "Epoch 27/350\n",
      "91/91 - 0s - loss: 11669115.0495 - mae: 2801.3464 - val_loss: 16175333.0000 - val_mae: 3204.9631\n",
      "Epoch 28/350\n",
      "91/91 - 0s - loss: 12583225.3846 - mae: 2807.7971 - val_loss: 11908055.0000 - val_mae: 2757.9668\n",
      "Epoch 29/350\n",
      "91/91 - 0s - loss: 12150836.6374 - mae: 2840.8660 - val_loss: 11571216.0000 - val_mae: 2681.9924\n",
      "Epoch 30/350\n",
      "91/91 - 0s - loss: 10371339.9011 - mae: 2639.5828 - val_loss: 15961187.0000 - val_mae: 3121.3828\n",
      "Epoch 31/350\n",
      "91/91 - 0s - loss: 10676691.6484 - mae: 2663.6780 - val_loss: 12062019.0000 - val_mae: 3062.4226\n",
      "Epoch 32/350\n",
      "91/91 - 0s - loss: 10913717.5165 - mae: 2671.8413 - val_loss: 10365241.0000 - val_mae: 2700.7659\n",
      "Epoch 33/350\n",
      "91/91 - 0s - loss: 9697974.4780 - mae: 2581.4021 - val_loss: 10293552.0000 - val_mae: 2593.2307\n",
      "Epoch 34/350\n",
      "91/91 - 0s - loss: 9090533.8407 - mae: 2458.7000 - val_loss: 11701965.0000 - val_mae: 3078.6797\n",
      "Epoch 35/350\n",
      "91/91 - 0s - loss: 9516167.8462 - mae: 2493.4915 - val_loss: 11369205.0000 - val_mae: 2734.0754\n",
      "Epoch 36/350\n",
      "91/91 - 0s - loss: 9265634.8352 - mae: 2437.6790 - val_loss: 11411064.0000 - val_mae: 2731.9824\n",
      "Epoch 37/350\n",
      "91/91 - 0s - loss: 9162427.5879 - mae: 2456.7188 - val_loss: 11167261.0000 - val_mae: 2860.7512\n",
      "Epoch 38/350\n",
      "91/91 - 0s - loss: 9136790.2527 - mae: 2477.9419 - val_loss: 10670760.0000 - val_mae: 2689.5818\n",
      "Epoch 39/350\n",
      "91/91 - 0s - loss: 9027258.7747 - mae: 2455.4163 - val_loss: 10726398.0000 - val_mae: 2703.3101\n",
      "Epoch 40/350\n",
      "91/91 - 0s - loss: 8888845.0769 - mae: 2440.8105 - val_loss: 10757193.0000 - val_mae: 2712.3740\n",
      "Epoch 41/350\n",
      "91/91 - 0s - loss: 9005952.0220 - mae: 2466.2678 - val_loss: 10688917.0000 - val_mae: 2663.0066\n",
      "Epoch 42/350\n",
      "91/91 - 0s - loss: 9300943.8132 - mae: 2510.3752 - val_loss: 10662801.0000 - val_mae: 2676.8049\n",
      "Epoch 43/350\n",
      "91/91 - 0s - loss: 9880209.8352 - mae: 2564.9944 - val_loss: 11334177.0000 - val_mae: 2908.4309\n",
      "Epoch 44/350\n",
      "91/91 - 0s - loss: 9224493.8242 - mae: 2428.9719 - val_loss: 11802325.0000 - val_mae: 2615.8799\n",
      "Epoch 45/350\n",
      "91/91 - 0s - loss: 8628359.2637 - mae: 2394.4243 - val_loss: 12821317.0000 - val_mae: 3163.7197\n",
      "Epoch 46/350\n",
      "91/91 - 0s - loss: 8620586.5549 - mae: 2421.1189 - val_loss: 10937195.0000 - val_mae: 2627.4836\n",
      "Epoch 47/350\n",
      "91/91 - 0s - loss: 8804863.4176 - mae: 2397.0125 - val_loss: 10787509.0000 - val_mae: 2680.4724\n",
      "Epoch 48/350\n",
      "91/91 - 0s - loss: 8804904.1209 - mae: 2388.2878 - val_loss: 11554179.0000 - val_mae: 2930.8860\n",
      "Epoch 49/350\n",
      "91/91 - 0s - loss: 8853658.6374 - mae: 2419.6975 - val_loss: 11935952.0000 - val_mae: 2620.4204\n",
      "Epoch 50/350\n",
      "91/91 - 0s - loss: 8014717.1978 - mae: 2353.1152 - val_loss: 13785112.0000 - val_mae: 3269.1990\n",
      "Epoch 51/350\n",
      "91/91 - 0s - loss: 8861681.6813 - mae: 2406.2046 - val_loss: 11104112.0000 - val_mae: 2611.3076\n",
      "Epoch 52/350\n",
      "91/91 - 0s - loss: 8453815.1374 - mae: 2360.0112 - val_loss: 10447914.0000 - val_mae: 2641.4026\n",
      "Epoch 53/350\n",
      "91/91 - 0s - loss: 8209942.3242 - mae: 2351.8494 - val_loss: 11145897.0000 - val_mae: 2867.0410\n",
      "Epoch 54/350\n",
      "91/91 - 0s - loss: 7737781.7308 - mae: 2310.3909 - val_loss: 11490675.0000 - val_mae: 2605.5051\n",
      "Epoch 55/350\n",
      "91/91 - 0s - loss: 8385090.7857 - mae: 2330.9209 - val_loss: 10937145.0000 - val_mae: 2713.9714\n",
      "Epoch 56/350\n",
      "91/91 - 0s - loss: 8106605.0659 - mae: 2381.0024 - val_loss: 11326584.0000 - val_mae: 2751.2402\n",
      "Epoch 57/350\n",
      "91/91 - 0s - loss: 8204300.4341 - mae: 2397.6267 - val_loss: 11327759.0000 - val_mae: 2624.6101\n",
      "Epoch 58/350\n",
      "91/91 - 0s - loss: 7912165.7363 - mae: 2284.2349 - val_loss: 12692391.0000 - val_mae: 3053.9248\n",
      "Epoch 59/350\n",
      "91/91 - 0s - loss: 8362733.8187 - mae: 2361.8770 - val_loss: 11162723.0000 - val_mae: 2589.7090\n",
      "Epoch 60/350\n",
      "91/91 - 0s - loss: 7637380.6703 - mae: 2241.8474 - val_loss: 11874935.0000 - val_mae: 2951.8953\n",
      "Epoch 61/350\n",
      "91/91 - 0s - loss: 8103247.3626 - mae: 2364.2979 - val_loss: 10396240.0000 - val_mae: 2610.5913\n",
      "Epoch 62/350\n",
      "91/91 - 0s - loss: 8547539.1319 - mae: 2344.6013 - val_loss: 10421021.0000 - val_mae: 2616.6660\n",
      "Epoch 63/350\n",
      "91/91 - 0s - loss: 8944476.8297 - mae: 2451.1521 - val_loss: 11025340.0000 - val_mae: 2756.4912\n",
      "Epoch 64/350\n",
      "91/91 - 0s - loss: 9534908.1758 - mae: 2535.1404 - val_loss: 10984919.0000 - val_mae: 2579.8376\n",
      "Epoch 65/350\n",
      "91/91 - 0s - loss: 7944594.4725 - mae: 2266.2625 - val_loss: 13994504.0000 - val_mae: 3275.8235\n",
      "Epoch 66/350\n",
      "91/91 - 0s - loss: 8024896.6978 - mae: 2348.6362 - val_loss: 12313619.0000 - val_mae: 2585.3535\n",
      "Epoch 67/350\n",
      "91/91 - 0s - loss: 8427106.6593 - mae: 2348.6899 - val_loss: 11633760.0000 - val_mae: 2870.7493\n",
      "Epoch 68/350\n",
      "91/91 - 0s - loss: 7788764.2747 - mae: 2278.5186 - val_loss: 10600479.0000 - val_mae: 2620.5808\n",
      "Epoch 69/350\n",
      "91/91 - 0s - loss: 7476001.3956 - mae: 2211.3743 - val_loss: 10608916.0000 - val_mae: 2663.4885\n",
      "Epoch 70/350\n",
      "91/91 - 0s - loss: 7582779.4780 - mae: 2260.2034 - val_loss: 10442757.0000 - val_mae: 2595.1062\n",
      "Epoch 71/350\n",
      "91/91 - 0s - loss: 8351802.0110 - mae: 2312.6604 - val_loss: 11139667.0000 - val_mae: 2783.0273\n",
      "Epoch 72/350\n",
      "91/91 - 0s - loss: 9421303.3516 - mae: 2468.6467 - val_loss: 10498353.0000 - val_mae: 2588.8801\n",
      "Epoch 73/350\n",
      "91/91 - 0s - loss: 8001064.9835 - mae: 2266.8545 - val_loss: 11615352.0000 - val_mae: 2560.1414\n",
      "Epoch 74/350\n",
      "91/91 - 0s - loss: 7234976.3462 - mae: 2193.6033 - val_loss: 13569920.0000 - val_mae: 3236.0286\n",
      "Epoch 75/350\n",
      "91/91 - 0s - loss: 9450538.5879 - mae: 2496.1821 - val_loss: 11724380.0000 - val_mae: 2563.8555\n",
      "Epoch 76/350\n",
      "91/91 - 0s - loss: 8609594.0934 - mae: 2355.3997 - val_loss: 10412497.0000 - val_mae: 2582.3535\n",
      "Epoch 77/350\n",
      "91/91 - 0s - loss: 7289550.8407 - mae: 2187.3789 - val_loss: 10952048.0000 - val_mae: 2723.3088\n",
      "Epoch 78/350\n",
      "91/91 - 0s - loss: 7489465.8242 - mae: 2206.8289 - val_loss: 10987769.0000 - val_mae: 2661.2324\n",
      "Epoch 79/350\n",
      "91/91 - 0s - loss: 7219803.2418 - mae: 2174.4685 - val_loss: 11382677.0000 - val_mae: 2735.8337\n",
      "Epoch 80/350\n",
      "91/91 - 0s - loss: 7603729.9505 - mae: 2242.9250 - val_loss: 11650643.0000 - val_mae: 2608.8645\n",
      "Epoch 81/350\n",
      "91/91 - 0s - loss: 8491400.9451 - mae: 2376.8960 - val_loss: 11525195.0000 - val_mae: 2776.1169\n",
      "Epoch 82/350\n",
      "91/91 - 0s - loss: 8608839.3901 - mae: 2436.8132 - val_loss: 10971846.0000 - val_mae: 2599.0081\n",
      "Epoch 83/350\n",
      "91/91 - 0s - loss: 9021778.0330 - mae: 2392.7422 - val_loss: 10469831.0000 - val_mae: 2564.1755\n",
      "Epoch 84/350\n",
      "91/91 - 0s - loss: 7546492.5165 - mae: 2171.2317 - val_loss: 12815491.0000 - val_mae: 3128.3164\n",
      "Epoch 85/350\n",
      "91/91 - 0s - loss: 7601403.3901 - mae: 2196.1978 - val_loss: 12894189.0000 - val_mae: 2687.2480\n",
      "Epoch 86/350\n",
      "91/91 - 0s - loss: 8651077.2637 - mae: 2305.1262 - val_loss: 11326453.0000 - val_mae: 2812.4333\n",
      "Epoch 87/350\n",
      "91/91 - 0s - loss: 8330407.9560 - mae: 2391.7109 - val_loss: 10482292.0000 - val_mae: 2618.1912\n",
      "Epoch 88/350\n",
      "91/91 - 0s - loss: 8551432.7033 - mae: 2417.6011 - val_loss: 10734485.0000 - val_mae: 2543.2422\n",
      "Epoch 89/350\n",
      "91/91 - 0s - loss: 8121180.9780 - mae: 2270.3877 - val_loss: 11773496.0000 - val_mae: 2906.8503\n",
      "Epoch 90/350\n",
      "91/91 - 0s - loss: 7814376.1429 - mae: 2207.6394 - val_loss: 11591527.0000 - val_mae: 2577.9724\n",
      "Epoch 91/350\n",
      "91/91 - 0s - loss: 8596173.6868 - mae: 2390.6079 - val_loss: 11266037.0000 - val_mae: 2824.6331\n",
      "Epoch 92/350\n",
      "91/91 - 0s - loss: 7637523.2088 - mae: 2236.7019 - val_loss: 10798489.0000 - val_mae: 2534.8459\n",
      "Epoch 93/350\n",
      "91/91 - 0s - loss: 7406295.8462 - mae: 2207.5356 - val_loss: 11218595.0000 - val_mae: 2716.0378\n",
      "Epoch 94/350\n",
      "91/91 - 0s - loss: 7225356.4505 - mae: 2149.6157 - val_loss: 11165251.0000 - val_mae: 2611.6907\n",
      "Epoch 95/350\n",
      "91/91 - 0s - loss: 7622958.2637 - mae: 2248.4807 - val_loss: 11133475.0000 - val_mae: 2631.5088\n",
      "Epoch 96/350\n",
      "91/91 - 0s - loss: 7663149.2308 - mae: 2216.9097 - val_loss: 11119711.0000 - val_mae: 2611.9146\n",
      "Epoch 97/350\n",
      "91/91 - 0s - loss: 7553547.4505 - mae: 2204.1851 - val_loss: 11098113.0000 - val_mae: 2679.5820\n",
      "Epoch 98/350\n",
      "91/91 - 0s - loss: 7191593.3681 - mae: 2163.2844 - val_loss: 11101072.0000 - val_mae: 2604.5840\n",
      "Epoch 99/350\n",
      "91/91 - 0s - loss: 7074952.8407 - mae: 2132.0176 - val_loss: 11489904.0000 - val_mae: 2780.0437\n",
      "Epoch 100/350\n",
      "91/91 - 0s - loss: 7282068.0934 - mae: 2210.3960 - val_loss: 11382697.0000 - val_mae: 2583.0178\n",
      "Epoch 101/350\n",
      "91/91 - 0s - loss: 8624867.7473 - mae: 2310.3496 - val_loss: 11144083.0000 - val_mae: 2714.7820\n",
      "Epoch 102/350\n",
      "91/91 - 0s - loss: 7497260.8187 - mae: 2248.9211 - val_loss: 11229781.0000 - val_mae: 2731.7246\n",
      "Epoch 103/350\n",
      "91/91 - 0s - loss: 7282235.2527 - mae: 2145.1111 - val_loss: 11293541.0000 - val_mae: 2566.5266\n",
      "Epoch 104/350\n",
      "91/91 - 0s - loss: 7214263.9505 - mae: 2134.5259 - val_loss: 11448439.0000 - val_mae: 2800.6316\n",
      "Epoch 105/350\n",
      "91/91 - 0s - loss: 7276160.6374 - mae: 2165.5457 - val_loss: 10599345.0000 - val_mae: 2535.4817\n",
      "Epoch 106/350\n",
      "91/91 - 0s - loss: 7193621.4176 - mae: 2141.6860 - val_loss: 10291934.0000 - val_mae: 2603.1145\n",
      "Epoch 107/350\n",
      "91/91 - 0s - loss: 7379292.0934 - mae: 2212.8916 - val_loss: 10456311.0000 - val_mae: 2575.0115\n",
      "Epoch 108/350\n",
      "91/91 - 0s - loss: 6986385.1209 - mae: 2125.9512 - val_loss: 11021505.0000 - val_mae: 2580.9050\n",
      "Epoch 109/350\n",
      "91/91 - 0s - loss: 7155414.1429 - mae: 2176.0767 - val_loss: 10938113.0000 - val_mae: 2588.1201\n",
      "Epoch 110/350\n",
      "91/91 - 0s - loss: 7224058.2747 - mae: 2163.8296 - val_loss: 10961519.0000 - val_mae: 2647.0369\n",
      "Epoch 111/350\n",
      "91/91 - 0s - loss: 7412454.3516 - mae: 2225.6277 - val_loss: 10883533.0000 - val_mae: 2581.7849\n",
      "Epoch 112/350\n",
      "91/91 - 0s - loss: 6970338.8187 - mae: 2117.5664 - val_loss: 11040364.0000 - val_mae: 2565.9661\n",
      "Epoch 113/350\n",
      "91/91 - 0s - loss: 7227321.4396 - mae: 2198.1865 - val_loss: 10914407.0000 - val_mae: 2645.9812\n",
      "Epoch 114/350\n",
      "91/91 - 0s - loss: 6812940.7912 - mae: 2122.2104 - val_loss: 11735613.0000 - val_mae: 2608.6750\n",
      "Epoch 115/350\n",
      "91/91 - 0s - loss: 7286516.5385 - mae: 2179.8838 - val_loss: 10950016.0000 - val_mae: 2663.3245\n",
      "Epoch 116/350\n",
      "91/91 - 0s - loss: 7116488.0769 - mae: 2186.9893 - val_loss: 10845007.0000 - val_mae: 2568.7107\n",
      "Epoch 117/350\n",
      "91/91 - 0s - loss: 7747326.9341 - mae: 2216.9832 - val_loss: 10772017.0000 - val_mae: 2580.7949\n",
      "Epoch 118/350\n",
      "91/91 - 0s - loss: 7690291.1868 - mae: 2234.3235 - val_loss: 10847533.0000 - val_mae: 2641.6697\n",
      "Epoch 119/350\n",
      "91/91 - 0s - loss: 8296525.0659 - mae: 2304.5647 - val_loss: 10820744.0000 - val_mae: 2561.4148\n",
      "Epoch 120/350\n",
      "91/91 - 0s - loss: 7143389.8736 - mae: 2138.7751 - val_loss: 12367616.0000 - val_mae: 3008.3623\n",
      "Epoch 121/350\n",
      "91/91 - 0s - loss: 8055942.3077 - mae: 2249.9695 - val_loss: 11772632.0000 - val_mae: 2618.5476\n",
      "Epoch 122/350\n",
      "91/91 - 0s - loss: 7173697.5165 - mae: 2110.7485 - val_loss: 11242421.0000 - val_mae: 2777.6455\n",
      "Epoch 123/350\n",
      "91/91 - 0s - loss: 7562018.3516 - mae: 2245.5449 - val_loss: 10552324.0000 - val_mae: 2543.4822\n",
      "Epoch 124/350\n",
      "91/91 - 0s - loss: 7335048.6813 - mae: 2147.3550 - val_loss: 10819174.0000 - val_mae: 2566.0085\n",
      "Epoch 125/350\n",
      "91/91 - 0s - loss: 7137156.4231 - mae: 2210.6809 - val_loss: 10856116.0000 - val_mae: 2606.8606\n",
      "Epoch 126/350\n",
      "91/91 - 0s - loss: 7045846.8791 - mae: 2156.9768 - val_loss: 11013341.0000 - val_mae: 2575.9683\n",
      "Epoch 127/350\n",
      "91/91 - 0s - loss: 6943203.8681 - mae: 2171.0020 - val_loss: 10991071.0000 - val_mae: 2642.8069\n",
      "Epoch 128/350\n",
      "91/91 - 0s - loss: 7218857.6813 - mae: 2159.2942 - val_loss: 10909269.0000 - val_mae: 2567.3821\n",
      "Epoch 129/350\n",
      "91/91 - 0s - loss: 6773368.6484 - mae: 2113.8845 - val_loss: 10995225.0000 - val_mae: 2638.5242\n",
      "Epoch 130/350\n",
      "91/91 - 0s - loss: 6794382.2418 - mae: 2123.3125 - val_loss: 11291973.0000 - val_mae: 2604.2537\n",
      "Epoch 131/350\n",
      "91/91 - 0s - loss: 6949932.5330 - mae: 2093.6780 - val_loss: 10830110.0000 - val_mae: 2605.7158\n",
      "Epoch 132/350\n",
      "91/91 - 0s - loss: 7558177.0989 - mae: 2258.9036 - val_loss: 10792147.0000 - val_mae: 2568.9812\n",
      "Epoch 133/350\n",
      "91/91 - 0s - loss: 7415561.1264 - mae: 2184.9854 - val_loss: 10524939.0000 - val_mae: 2551.7188\n",
      "Epoch 134/350\n",
      "91/91 - 0s - loss: 7604232.3077 - mae: 2187.6296 - val_loss: 10924771.0000 - val_mae: 2659.4109\n",
      "Epoch 135/350\n",
      "91/91 - 0s - loss: 8511750.7802 - mae: 2371.9419 - val_loss: 11172958.0000 - val_mae: 2592.0491\n",
      "Epoch 136/350\n",
      "91/91 - 0s - loss: 8187961.8791 - mae: 2287.7229 - val_loss: 11270701.0000 - val_mae: 2772.9089\n",
      "Epoch 137/350\n",
      "91/91 - 0s - loss: 8428165.5495 - mae: 2260.9641 - val_loss: 11974768.0000 - val_mae: 2671.9099\n",
      "Epoch 138/350\n",
      "91/91 - 0s - loss: 9412544.9451 - mae: 2422.3999 - val_loss: 10786808.0000 - val_mae: 2652.2341\n",
      "Epoch 139/350\n",
      "91/91 - 0s - loss: 6976398.0769 - mae: 2142.7727 - val_loss: 12133624.0000 - val_mae: 2627.9983\n",
      "Epoch 140/350\n",
      "91/91 - 0s - loss: 7447380.5495 - mae: 2186.3635 - val_loss: 11849941.0000 - val_mae: 2854.9153\n",
      "Epoch 141/350\n",
      "91/91 - 0s - loss: 7323743.7198 - mae: 2189.0134 - val_loss: 11044901.0000 - val_mae: 2588.2498\n",
      "Epoch 142/350\n",
      "91/91 - 0s - loss: 6848599.6154 - mae: 2111.2334 - val_loss: 10643565.0000 - val_mae: 2542.5764\n",
      "Epoch 143/350\n",
      "91/91 - 0s - loss: 6694381.6264 - mae: 2079.3833 - val_loss: 10569859.0000 - val_mae: 2539.5061\n",
      "Epoch 144/350\n",
      "91/91 - 0s - loss: 6776195.3736 - mae: 2089.4707 - val_loss: 10500355.0000 - val_mae: 2540.1008\n",
      "Epoch 145/350\n",
      "91/91 - 0s - loss: 6788717.1154 - mae: 2088.5083 - val_loss: 10509967.0000 - val_mae: 2538.5906\n",
      "Epoch 146/350\n",
      "91/91 - 0s - loss: 6756200.7363 - mae: 2079.0234 - val_loss: 10606163.0000 - val_mae: 2554.5491\n",
      "Epoch 147/350\n",
      "91/91 - 0s - loss: 6736298.5000 - mae: 2083.2021 - val_loss: 10615985.0000 - val_mae: 2555.0737\n",
      "Epoch 148/350\n",
      "91/91 - 0s - loss: 6694947.3516 - mae: 2074.3076 - val_loss: 10686944.0000 - val_mae: 2598.4343\n",
      "Epoch 149/350\n",
      "91/91 - 0s - loss: 6855573.7637 - mae: 2109.7734 - val_loss: 10547635.0000 - val_mae: 2540.5999\n",
      "Epoch 150/350\n",
      "91/91 - 0s - loss: 6731490.5165 - mae: 2091.8096 - val_loss: 10547702.0000 - val_mae: 2544.1267\n",
      "Epoch 151/350\n",
      "91/91 - 0s - loss: 6782860.7637 - mae: 2097.1479 - val_loss: 10470603.0000 - val_mae: 2552.5457\n",
      "Epoch 152/350\n",
      "91/91 - 0s - loss: 6620818.7527 - mae: 2058.4333 - val_loss: 10606227.0000 - val_mae: 2579.3455\n",
      "Epoch 153/350\n",
      "91/91 - 0s - loss: 6703848.9560 - mae: 2099.4783 - val_loss: 10519995.0000 - val_mae: 2552.4717\n",
      "Epoch 154/350\n",
      "91/91 - 0s - loss: 6705569.9451 - mae: 2074.7007 - val_loss: 10733608.0000 - val_mae: 2581.5391\n",
      "Epoch 155/350\n",
      "91/91 - 0s - loss: 6888103.2473 - mae: 2089.9534 - val_loss: 10563995.0000 - val_mae: 2537.5369\n",
      "Epoch 156/350\n",
      "91/91 - 0s - loss: 6618121.4341 - mae: 2073.8088 - val_loss: 10594832.0000 - val_mae: 2570.4509\n",
      "Epoch 157/350\n",
      "91/91 - 0s - loss: 7115602.2637 - mae: 2149.4299 - val_loss: 10456290.0000 - val_mae: 2534.3584\n",
      "Epoch 158/350\n",
      "91/91 - 0s - loss: 6970103.8407 - mae: 2148.5625 - val_loss: 10542511.0000 - val_mae: 2564.3059\n",
      "Epoch 159/350\n",
      "91/91 - 0s - loss: 6889700.8022 - mae: 2103.6396 - val_loss: 10968504.0000 - val_mae: 2596.8792\n",
      "Epoch 160/350\n",
      "91/91 - 0s - loss: 7377765.5769 - mae: 2206.8040 - val_loss: 10745964.0000 - val_mae: 2612.1941\n",
      "Epoch 161/350\n",
      "91/91 - 0s - loss: 7329728.7308 - mae: 2176.0764 - val_loss: 10808482.0000 - val_mae: 2591.2637\n",
      "Epoch 162/350\n",
      "91/91 - 0s - loss: 6573594.0659 - mae: 2115.1230 - val_loss: 11387312.0000 - val_mae: 2747.1726\n",
      "Epoch 163/350\n",
      "91/91 - 0s - loss: 6823053.1538 - mae: 2118.1262 - val_loss: 11292608.0000 - val_mae: 2616.1169\n",
      "Epoch 164/350\n",
      "91/91 - 0s - loss: 7673109.5604 - mae: 2147.7012 - val_loss: 10509048.0000 - val_mae: 2565.4685\n",
      "Epoch 165/350\n",
      "91/91 - 0s - loss: 9104546.5879 - mae: 2440.3689 - val_loss: 10350499.0000 - val_mae: 2562.0417\n",
      "Epoch 166/350\n",
      "91/91 - 0s - loss: 8843693.4231 - mae: 2323.7251 - val_loss: 10064511.0000 - val_mae: 2544.8340\n",
      "Epoch 167/350\n",
      "91/91 - 0s - loss: 8750079.3077 - mae: 2448.7095 - val_loss: 10287941.0000 - val_mae: 2581.9109\n",
      "Epoch 168/350\n",
      "91/91 - 0s - loss: 8265473.6264 - mae: 2391.6838 - val_loss: 12188029.0000 - val_mae: 2777.4568\n",
      "Epoch 169/350\n",
      "91/91 - 0s - loss: 6913682.4231 - mae: 2074.0884 - val_loss: 12917373.0000 - val_mae: 3073.8301\n",
      "Epoch 170/350\n",
      "91/91 - 0s - loss: 7882801.9341 - mae: 2259.8911 - val_loss: 11332739.0000 - val_mae: 2626.5815\n",
      "Epoch 171/350\n",
      "91/91 - 0s - loss: 9004243.2912 - mae: 2345.8208 - val_loss: 10485820.0000 - val_mae: 2546.4543\n",
      "Epoch 172/350\n",
      "91/91 - 0s - loss: 7432964.9780 - mae: 2211.7107 - val_loss: 10377972.0000 - val_mae: 2527.0701\n",
      "Epoch 173/350\n",
      "91/91 - 0s - loss: 6684731.2363 - mae: 2040.6530 - val_loss: 11096946.0000 - val_mae: 2615.4932\n",
      "Epoch 174/350\n",
      "91/91 - 0s - loss: 6930378.8462 - mae: 2135.7544 - val_loss: 10893659.0000 - val_mae: 2638.8213\n",
      "Epoch 175/350\n",
      "91/91 - 0s - loss: 7738979.5714 - mae: 2213.0427 - val_loss: 10646508.0000 - val_mae: 2590.7000\n",
      "Epoch 176/350\n",
      "91/91 - 0s - loss: 7297077.3242 - mae: 2226.2000 - val_loss: 10569347.0000 - val_mae: 2571.6204\n",
      "Epoch 177/350\n",
      "91/91 - 0s - loss: 6692320.8516 - mae: 2053.3474 - val_loss: 11349188.0000 - val_mae: 2635.3196\n",
      "Epoch 178/350\n",
      "91/91 - 0s - loss: 7287482.5110 - mae: 2105.3455 - val_loss: 10826477.0000 - val_mae: 2641.8181\n",
      "Epoch 179/350\n",
      "91/91 - 0s - loss: 6813492.9176 - mae: 2114.4336 - val_loss: 11156137.0000 - val_mae: 2616.4514\n",
      "Epoch 180/350\n",
      "91/91 - 0s - loss: 8031720.9121 - mae: 2175.7683 - val_loss: 10405953.0000 - val_mae: 2545.6848\n",
      "Epoch 181/350\n",
      "91/91 - 0s - loss: 6715475.3901 - mae: 2084.2139 - val_loss: 10681779.0000 - val_mae: 2587.5862\n",
      "Epoch 182/350\n",
      "91/91 - 0s - loss: 6515182.5934 - mae: 2063.9299 - val_loss: 11437875.0000 - val_mae: 2631.2061\n",
      "Epoch 183/350\n",
      "91/91 - 0s - loss: 7552397.5110 - mae: 2140.7371 - val_loss: 10954227.0000 - val_mae: 2655.4783\n",
      "Epoch 184/350\n",
      "91/91 - 0s - loss: 7727621.1758 - mae: 2262.3777 - val_loss: 10196216.0000 - val_mae: 2530.7625\n",
      "Epoch 185/350\n",
      "91/91 - 0s - loss: 7645276.4670 - mae: 2168.4871 - val_loss: 10452549.0000 - val_mae: 2577.6208\n",
      "Epoch 186/350\n",
      "91/91 - 0s - loss: 8802398.8846 - mae: 2362.9668 - val_loss: 10316683.0000 - val_mae: 2529.3054\n",
      "Epoch 187/350\n",
      "91/91 - 0s - loss: 8042992.2418 - mae: 2187.7812 - val_loss: 10906389.0000 - val_mae: 2607.0730\n",
      "Epoch 188/350\n",
      "91/91 - 0s - loss: 7267315.7308 - mae: 2201.2637 - val_loss: 11289737.0000 - val_mae: 2747.7532\n",
      "Epoch 189/350\n",
      "91/91 - 0s - loss: 6790643.4560 - mae: 2121.7590 - val_loss: 11414773.0000 - val_mae: 2648.3728\n",
      "Epoch 190/350\n",
      "91/91 - 0s - loss: 7051432.6099 - mae: 2071.2063 - val_loss: 10651877.0000 - val_mae: 2608.6077\n",
      "Epoch 191/350\n",
      "91/91 - 0s - loss: 7225397.5824 - mae: 2165.2703 - val_loss: 10160201.0000 - val_mae: 2548.0732\n",
      "Epoch 192/350\n",
      "91/91 - 0s - loss: 7025127.5769 - mae: 2074.6113 - val_loss: 10434555.0000 - val_mae: 2587.1199\n",
      "Epoch 193/350\n",
      "91/91 - 0s - loss: 6791275.6374 - mae: 2125.2136 - val_loss: 11011688.0000 - val_mae: 2669.2473\n",
      "Epoch 194/350\n",
      "91/91 - 0s - loss: 6981120.5604 - mae: 2089.8469 - val_loss: 11121891.0000 - val_mae: 2625.1982\n",
      "Epoch 195/350\n",
      "91/91 - 0s - loss: 6984799.7198 - mae: 2111.8962 - val_loss: 11000937.0000 - val_mae: 2672.2434\n",
      "Epoch 196/350\n",
      "91/91 - 0s - loss: 7194601.1813 - mae: 2181.0466 - val_loss: 10401063.0000 - val_mae: 2585.9500\n",
      "Epoch 197/350\n",
      "91/91 - 0s - loss: 6465744.3516 - mae: 2032.3508 - val_loss: 10342857.0000 - val_mae: 2540.4587\n",
      "Epoch 198/350\n",
      "91/91 - 0s - loss: 6838146.8462 - mae: 2100.9326 - val_loss: 10705064.0000 - val_mae: 2608.3767\n",
      "Epoch 199/350\n",
      "91/91 - 0s - loss: 6945033.3462 - mae: 2074.2729 - val_loss: 10188593.0000 - val_mae: 2545.0076\n",
      "Epoch 200/350\n",
      "91/91 - 0s - loss: 7042093.3297 - mae: 2166.5647 - val_loss: 10237293.0000 - val_mae: 2524.2034\n",
      "Epoch 201/350\n",
      "91/91 - 0s - loss: 6466109.8352 - mae: 2009.0575 - val_loss: 10536908.0000 - val_mae: 2596.1038\n",
      "Epoch 202/350\n",
      "91/91 - 0s - loss: 6607291.8407 - mae: 2046.2905 - val_loss: 10264208.0000 - val_mae: 2524.7151\n",
      "Epoch 203/350\n",
      "91/91 - 0s - loss: 7015315.5879 - mae: 2126.5962 - val_loss: 10278565.0000 - val_mae: 2580.7136\n",
      "Epoch 204/350\n",
      "91/91 - 0s - loss: 8286276.1099 - mae: 2311.5588 - val_loss: 9889837.0000 - val_mae: 2545.8857\n",
      "Epoch 205/350\n",
      "91/91 - 0s - loss: 8689380.0440 - mae: 2392.2332 - val_loss: 10065417.0000 - val_mae: 2512.6956\n",
      "Epoch 206/350\n",
      "91/91 - 0s - loss: 7870931.5055 - mae: 2217.8989 - val_loss: 11285893.0000 - val_mae: 2737.1267\n",
      "Epoch 207/350\n",
      "91/91 - 0s - loss: 6966578.1264 - mae: 2040.8197 - val_loss: 11052987.0000 - val_mae: 2726.7617\n",
      "Epoch 208/350\n",
      "91/91 - 0s - loss: 7083489.8132 - mae: 2168.2849 - val_loss: 10833917.0000 - val_mae: 2636.8240\n",
      "Epoch 209/350\n",
      "91/91 - 0s - loss: 8200615.4066 - mae: 2181.7605 - val_loss: 10132743.0000 - val_mae: 2514.8987\n",
      "Epoch 210/350\n",
      "91/91 - 0s - loss: 7045905.3352 - mae: 2145.8469 - val_loss: 10769443.0000 - val_mae: 2610.1125\n",
      "Epoch 211/350\n",
      "91/91 - 0s - loss: 8102474.4945 - mae: 2278.6101 - val_loss: 11161521.0000 - val_mae: 2635.3420\n",
      "Epoch 212/350\n",
      "91/91 - 0s - loss: 6478161.9286 - mae: 1939.3484 - val_loss: 11743989.0000 - val_mae: 2845.0598\n",
      "Epoch 213/350\n",
      "91/91 - 0s - loss: 6986564.2637 - mae: 2097.3054 - val_loss: 11340096.0000 - val_mae: 2661.1575\n",
      "Epoch 214/350\n",
      "91/91 - 0s - loss: 6931763.1429 - mae: 2059.6067 - val_loss: 10373419.0000 - val_mae: 2529.8757\n",
      "Epoch 215/350\n",
      "91/91 - 0s - loss: 7388388.8846 - mae: 2183.0581 - val_loss: 10217861.0000 - val_mae: 2575.8040\n",
      "Epoch 216/350\n",
      "91/91 - 0s - loss: 6604221.5055 - mae: 2058.1714 - val_loss: 10076082.0000 - val_mae: 2578.0193\n",
      "Epoch 217/350\n",
      "91/91 - 0s - loss: 6461700.6923 - mae: 2040.8962 - val_loss: 10141995.0000 - val_mae: 2464.6062\n",
      "Epoch 218/350\n",
      "91/91 - 0s - loss: 6643637.9945 - mae: 2090.4326 - val_loss: 10447576.0000 - val_mae: 2692.3132\n",
      "Epoch 219/350\n",
      "91/91 - 0s - loss: 7170536.5934 - mae: 2129.3511 - val_loss: 10323649.0000 - val_mae: 2644.1260\n",
      "Epoch 220/350\n",
      "91/91 - 0s - loss: 7090555.0385 - mae: 2115.6819 - val_loss: 8503355.0000 - val_mae: 2442.4199\n",
      "Epoch 221/350\n",
      "91/91 - 0s - loss: 7139338.4066 - mae: 2108.0691 - val_loss: 9322888.0000 - val_mae: 2467.4143\n",
      "Epoch 222/350\n",
      "91/91 - 0s - loss: 7144024.1923 - mae: 2137.7839 - val_loss: 9369691.0000 - val_mae: 2558.6748\n",
      "Epoch 223/350\n",
      "91/91 - 0s - loss: 7977365.2747 - mae: 2204.0344 - val_loss: 9063055.0000 - val_mae: 2387.4236\n",
      "Epoch 224/350\n",
      "91/91 - 0s - loss: 7765724.5055 - mae: 2274.0859 - val_loss: 8632248.0000 - val_mae: 2384.7380\n",
      "Epoch 225/350\n",
      "91/91 - 0s - loss: 8331987.5714 - mae: 2253.3435 - val_loss: 11267229.0000 - val_mae: 2824.2832\n",
      "Epoch 226/350\n",
      "91/91 - 0s - loss: 7554284.6044 - mae: 2214.9341 - val_loss: 10758382.0000 - val_mae: 2708.0549\n",
      "Epoch 227/350\n",
      "91/91 - 0s - loss: 7791368.3187 - mae: 2180.1941 - val_loss: 11597224.0000 - val_mae: 2891.3337\n",
      "Epoch 228/350\n",
      "91/91 - 0s - loss: 7938033.6703 - mae: 2233.7156 - val_loss: 10788868.0000 - val_mae: 2686.4397\n",
      "Epoch 229/350\n",
      "91/91 - 0s - loss: 7127444.6868 - mae: 2101.9949 - val_loss: 11123409.0000 - val_mae: 2774.8445\n",
      "Epoch 230/350\n",
      "91/91 - 0s - loss: 7206167.5989 - mae: 2100.6545 - val_loss: 10641116.0000 - val_mae: 2539.4099\n",
      "Epoch 231/350\n",
      "91/91 - 0s - loss: 6962734.8901 - mae: 2098.9771 - val_loss: 10745472.0000 - val_mae: 2690.8428\n",
      "Epoch 232/350\n",
      "91/91 - 0s - loss: 7172384.5714 - mae: 2137.1565 - val_loss: 10657055.0000 - val_mae: 2503.0864\n",
      "Epoch 233/350\n",
      "91/91 - 0s - loss: 7264939.2308 - mae: 2192.1680 - val_loss: 10745968.0000 - val_mae: 2734.9827\n",
      "Epoch 234/350\n",
      "91/91 - 0s - loss: 7549469.7308 - mae: 2139.2996 - val_loss: 9979908.0000 - val_mae: 2453.3762\n",
      "Epoch 235/350\n",
      "91/91 - 0s - loss: 7482587.5714 - mae: 2257.1382 - val_loss: 10642752.0000 - val_mae: 2569.3127\n",
      "Epoch 236/350\n",
      "91/91 - 0s - loss: 7184911.0000 - mae: 2155.9963 - val_loss: 10524443.0000 - val_mae: 2597.2537\n",
      "Epoch 237/350\n",
      "91/91 - 0s - loss: 7149843.0220 - mae: 2149.9385 - val_loss: 10450398.0000 - val_mae: 2551.9812\n",
      "Epoch 238/350\n",
      "91/91 - 0s - loss: 7243661.3736 - mae: 2192.1775 - val_loss: 10724060.0000 - val_mae: 2740.9600\n",
      "Epoch 239/350\n",
      "91/91 - 0s - loss: 7412554.6923 - mae: 2097.6150 - val_loss: 10151207.0000 - val_mae: 2629.6338\n",
      "Epoch 240/350\n",
      "91/91 - 0s - loss: 7504356.2418 - mae: 2192.1931 - val_loss: 10119927.0000 - val_mae: 2512.2056\n",
      "Epoch 241/350\n",
      "91/91 - 0s - loss: 7107355.2198 - mae: 2117.8843 - val_loss: 12177517.0000 - val_mae: 2947.0085\n",
      "Epoch 242/350\n",
      "91/91 - 0s - loss: 7276678.5604 - mae: 2090.7500 - val_loss: 11052743.0000 - val_mae: 2611.2129\n",
      "Epoch 243/350\n",
      "91/91 - 0s - loss: 7452790.2418 - mae: 2137.8472 - val_loss: 11278679.0000 - val_mae: 2780.4209\n",
      "Epoch 244/350\n",
      "91/91 - 0s - loss: 6765142.2088 - mae: 2075.5652 - val_loss: 10674777.0000 - val_mae: 2551.8694\n",
      "Epoch 245/350\n",
      "91/91 - 0s - loss: 6759513.2967 - mae: 2077.9771 - val_loss: 11253229.0000 - val_mae: 2781.1855\n",
      "Epoch 246/350\n",
      "91/91 - 0s - loss: 6534923.2582 - mae: 2007.4885 - val_loss: 10623917.0000 - val_mae: 2548.5720\n",
      "Epoch 247/350\n",
      "91/91 - 0s - loss: 7100606.9890 - mae: 2129.5891 - val_loss: 10931501.0000 - val_mae: 2715.1199\n",
      "Epoch 248/350\n",
      "91/91 - 0s - loss: 7430284.8681 - mae: 2063.7490 - val_loss: 10803358.0000 - val_mae: 2685.2458\n",
      "Epoch 249/350\n",
      "91/91 - 0s - loss: 7788993.8187 - mae: 2211.9387 - val_loss: 10437732.0000 - val_mae: 2534.3828\n",
      "Epoch 250/350\n",
      "91/91 - 0s - loss: 7105677.7912 - mae: 2079.8953 - val_loss: 11415216.0000 - val_mae: 2818.5164\n",
      "Epoch 251/350\n",
      "91/91 - 0s - loss: 7039850.7527 - mae: 2126.0193 - val_loss: 10627094.0000 - val_mae: 2542.4734\n",
      "Epoch 252/350\n",
      "91/91 - 0s - loss: 7395082.2308 - mae: 2106.5796 - val_loss: 11073283.0000 - val_mae: 2779.9036\n",
      "Epoch 253/350\n",
      "91/91 - 0s - loss: 7456136.9121 - mae: 2108.5588 - val_loss: 10437131.0000 - val_mae: 2511.2083\n",
      "Epoch 254/350\n",
      "91/91 - 0s - loss: 7178785.0220 - mae: 2128.9626 - val_loss: 11769293.0000 - val_mae: 2874.1038\n",
      "Epoch 255/350\n",
      "91/91 - 0s - loss: 7457718.8407 - mae: 2086.1140 - val_loss: 11325957.0000 - val_mae: 2717.9451\n",
      "Epoch 256/350\n",
      "91/91 - 0s - loss: 7185942.6264 - mae: 2098.7817 - val_loss: 13207952.0000 - val_mae: 3002.6692\n",
      "Epoch 257/350\n",
      "91/91 - 0s - loss: 7777565.7143 - mae: 2218.4077 - val_loss: 10635613.0000 - val_mae: 2543.3330\n",
      "Epoch 258/350\n",
      "91/91 - 0s - loss: 7133499.7198 - mae: 2131.1462 - val_loss: 13245747.0000 - val_mae: 3003.0925\n",
      "Epoch 259/350\n",
      "91/91 - 0s - loss: 7524609.6154 - mae: 2134.9792 - val_loss: 10059882.0000 - val_mae: 2493.4036\n",
      "Epoch 260/350\n",
      "91/91 - 0s - loss: 6726299.4945 - mae: 2077.0327 - val_loss: 10025500.0000 - val_mae: 2527.8975\n",
      "Epoch 261/350\n",
      "91/91 - 0s - loss: 6543232.1099 - mae: 2030.5021 - val_loss: 10174789.0000 - val_mae: 2645.7449\n",
      "Epoch 262/350\n",
      "91/91 - 0s - loss: 6896211.5275 - mae: 2106.5945 - val_loss: 10237997.0000 - val_mae: 2673.3152\n",
      "Epoch 263/350\n",
      "91/91 - 0s - loss: 6737730.0495 - mae: 2062.4355 - val_loss: 10251000.0000 - val_mae: 2669.1665\n",
      "Epoch 264/350\n",
      "91/91 - 0s - loss: 6564209.0495 - mae: 2026.3569 - val_loss: 10116605.0000 - val_mae: 2605.6777\n",
      "Epoch 265/350\n",
      "91/91 - 0s - loss: 6747719.1044 - mae: 2083.0994 - val_loss: 10219369.0000 - val_mae: 2655.5515\n",
      "Epoch 266/350\n",
      "91/91 - 0s - loss: 6482160.2747 - mae: 2011.1462 - val_loss: 9995300.0000 - val_mae: 2566.8396\n",
      "Epoch 267/350\n",
      "91/91 - 0s - loss: 6563772.3352 - mae: 2033.6238 - val_loss: 9946924.0000 - val_mae: 2567.8594\n",
      "Epoch 268/350\n",
      "91/91 - 0s - loss: 6506623.3132 - mae: 2032.2788 - val_loss: 9811082.0000 - val_mae: 2541.3345\n",
      "Epoch 269/350\n",
      "91/91 - 0s - loss: 7416895.8956 - mae: 2167.1572 - val_loss: 9766847.0000 - val_mae: 2537.8560\n",
      "Epoch 270/350\n",
      "91/91 - 0s - loss: 6553535.0659 - mae: 2036.1766 - val_loss: 10073360.0000 - val_mae: 2481.1628\n",
      "Epoch 271/350\n",
      "91/91 - 0s - loss: 6801730.7802 - mae: 2090.7588 - val_loss: 10049708.0000 - val_mae: 2642.6150\n",
      "Epoch 272/350\n",
      "91/91 - 0s - loss: 6626758.9780 - mae: 2060.9558 - val_loss: 10557906.0000 - val_mae: 2751.9700\n",
      "Epoch 273/350\n",
      "91/91 - 0s - loss: 6848971.5549 - mae: 2035.0148 - val_loss: 9698021.0000 - val_mae: 2519.2693\n",
      "Epoch 274/350\n",
      "91/91 - 0s - loss: 7370222.4176 - mae: 2215.0703 - val_loss: 9842241.0000 - val_mae: 2586.8672\n",
      "Epoch 275/350\n",
      "91/91 - 0s - loss: 7585384.3791 - mae: 2128.6028 - val_loss: 9937681.0000 - val_mae: 2643.1682\n",
      "Epoch 276/350\n",
      "91/91 - 0s - loss: 8076066.4615 - mae: 2286.7180 - val_loss: 9471409.0000 - val_mae: 2493.7161\n",
      "Epoch 277/350\n",
      "91/91 - 0s - loss: 7962212.9560 - mae: 2180.1565 - val_loss: 11737265.0000 - val_mae: 2935.1174\n",
      "Epoch 278/350\n",
      "91/91 - 0s - loss: 7653656.8187 - mae: 2248.8181 - val_loss: 10645852.0000 - val_mae: 2591.5447\n",
      "Epoch 279/350\n",
      "91/91 - 0s - loss: 6951378.0220 - mae: 2066.4668 - val_loss: 11711681.0000 - val_mae: 2900.3672\n",
      "Epoch 280/350\n",
      "91/91 - 0s - loss: 7382183.6813 - mae: 2116.8135 - val_loss: 9810695.0000 - val_mae: 2470.0613\n",
      "Epoch 281/350\n",
      "91/91 - 0s - loss: 6645032.9341 - mae: 2062.4285 - val_loss: 10029613.0000 - val_mae: 2650.3523\n",
      "Epoch 282/350\n",
      "91/91 - 0s - loss: 6571373.0275 - mae: 2027.3113 - val_loss: 9890908.0000 - val_mae: 2612.1130\n",
      "Epoch 283/350\n",
      "91/91 - 0s - loss: 6583701.8242 - mae: 2030.8795 - val_loss: 9696396.0000 - val_mae: 2498.8777\n",
      "Epoch 284/350\n",
      "91/91 - 0s - loss: 6449389.5714 - mae: 2009.7268 - val_loss: 10422840.0000 - val_mae: 2730.7288\n",
      "Epoch 285/350\n",
      "91/91 - 0s - loss: 6701430.0055 - mae: 2062.4314 - val_loss: 9695275.0000 - val_mae: 2494.7437\n",
      "Epoch 286/350\n",
      "91/91 - 0s - loss: 6349061.2418 - mae: 2001.1545 - val_loss: 10288030.0000 - val_mae: 2725.3914\n",
      "Epoch 287/350\n",
      "91/91 - 0s - loss: 6589206.6593 - mae: 2042.1710 - val_loss: 9544948.0000 - val_mae: 2499.1765\n",
      "Epoch 288/350\n",
      "91/91 - 0s - loss: 6685363.0769 - mae: 2055.0002 - val_loss: 9584098.0000 - val_mae: 2540.0288\n",
      "Epoch 289/350\n",
      "91/91 - 0s - loss: 6551999.3132 - mae: 2053.3533 - val_loss: 9655627.0000 - val_mae: 2573.0303\n",
      "Epoch 290/350\n",
      "91/91 - 0s - loss: 6617008.4121 - mae: 2026.3276 - val_loss: 9526892.0000 - val_mae: 2525.5398\n",
      "Epoch 291/350\n",
      "91/91 - 0s - loss: 6681913.3516 - mae: 2055.6423 - val_loss: 9533407.0000 - val_mae: 2526.7100\n",
      "Epoch 292/350\n",
      "91/91 - 0s - loss: 6527073.4615 - mae: 2070.6235 - val_loss: 9430465.0000 - val_mae: 2506.1018\n",
      "Epoch 293/350\n",
      "91/91 - 0s - loss: 6289252.2445 - mae: 2006.1440 - val_loss: 10879692.0000 - val_mae: 2806.2454\n",
      "Epoch 294/350\n",
      "91/91 - 0s - loss: 6877647.6154 - mae: 2034.6046 - val_loss: 9527367.0000 - val_mae: 2494.3928\n",
      "Epoch 295/350\n",
      "91/91 - 0s - loss: 6465518.5604 - mae: 2050.4465 - val_loss: 9530621.0000 - val_mae: 2466.7131\n",
      "Epoch 296/350\n",
      "91/91 - 0s - loss: 6684631.0220 - mae: 2059.5439 - val_loss: 9717241.0000 - val_mae: 2624.9453\n",
      "Epoch 297/350\n",
      "91/91 - 0s - loss: 6833108.1154 - mae: 2051.4910 - val_loss: 9670132.0000 - val_mae: 2455.6072\n",
      "Epoch 298/350\n",
      "91/91 - 0s - loss: 6788957.9615 - mae: 2088.1780 - val_loss: 9536604.0000 - val_mae: 2555.0061\n",
      "Epoch 299/350\n",
      "91/91 - 0s - loss: 6364286.2967 - mae: 2011.4320 - val_loss: 10414773.0000 - val_mae: 2746.9316\n",
      "Epoch 300/350\n",
      "91/91 - 0s - loss: 6526601.9670 - mae: 2050.0613 - val_loss: 9433072.0000 - val_mae: 2488.8938\n",
      "Epoch 301/350\n",
      "91/91 - 0s - loss: 6345333.4615 - mae: 1989.5898 - val_loss: 9726251.0000 - val_mae: 2622.6074\n",
      "Epoch 302/350\n",
      "91/91 - 0s - loss: 6670608.4341 - mae: 2019.5653 - val_loss: 9398476.0000 - val_mae: 2474.5505\n",
      "Epoch 303/350\n",
      "91/91 - 0s - loss: 7220171.2857 - mae: 2166.8867 - val_loss: 9627319.0000 - val_mae: 2600.6794\n",
      "Epoch 304/350\n",
      "91/91 - 0s - loss: 6674320.7582 - mae: 2005.3641 - val_loss: 10109451.0000 - val_mae: 2712.8533\n",
      "Epoch 305/350\n",
      "91/91 - 0s - loss: 6307829.5549 - mae: 2011.3485 - val_loss: 9721744.0000 - val_mae: 2462.5339\n",
      "Epoch 306/350\n",
      "91/91 - 0s - loss: 6781342.0659 - mae: 2073.7188 - val_loss: 9660658.0000 - val_mae: 2641.3875\n",
      "Epoch 307/350\n",
      "91/91 - 0s - loss: 6344331.0495 - mae: 1990.6830 - val_loss: 9452287.0000 - val_mae: 2465.0859\n",
      "Epoch 308/350\n",
      "91/91 - 0s - loss: 6764971.1813 - mae: 2100.1184 - val_loss: 9779335.0000 - val_mae: 2642.8679\n",
      "Epoch 309/350\n",
      "91/91 - 0s - loss: 6892484.3681 - mae: 2035.1439 - val_loss: 9406239.0000 - val_mae: 2523.4258\n",
      "Epoch 310/350\n",
      "91/91 - 0s - loss: 6257252.8571 - mae: 2005.5381 - val_loss: 9528137.0000 - val_mae: 2469.9954\n",
      "Epoch 311/350\n",
      "91/91 - 0s - loss: 6454192.0440 - mae: 2054.0657 - val_loss: 10237269.0000 - val_mae: 2727.5168\n",
      "Epoch 312/350\n",
      "91/91 - 0s - loss: 6648567.2802 - mae: 2007.1145 - val_loss: 9362511.0000 - val_mae: 2456.6213\n",
      "Epoch 313/350\n",
      "91/91 - 0s - loss: 6584935.9451 - mae: 2060.0984 - val_loss: 9601188.0000 - val_mae: 2627.2761\n",
      "Epoch 314/350\n",
      "91/91 - 0s - loss: 6563868.1484 - mae: 1992.1859 - val_loss: 9317019.0000 - val_mae: 2545.5259\n",
      "Epoch 315/350\n",
      "91/91 - 0s - loss: 6232977.6016 - mae: 1979.6760 - val_loss: 9473569.0000 - val_mae: 2449.2283\n",
      "Epoch 316/350\n",
      "91/91 - 0s - loss: 6618403.8407 - mae: 2021.1888 - val_loss: 10073215.0000 - val_mae: 2700.1865\n",
      "Epoch 317/350\n",
      "91/91 - 0s - loss: 6614171.0330 - mae: 2009.7770 - val_loss: 9473815.0000 - val_mae: 2449.1624\n",
      "Epoch 318/350\n",
      "91/91 - 0s - loss: 6365435.3626 - mae: 2039.2817 - val_loss: 10001313.0000 - val_mae: 2691.3113\n",
      "Epoch 319/350\n",
      "91/91 - 0s - loss: 6673142.0000 - mae: 2013.0841 - val_loss: 9306577.0000 - val_mae: 2462.3574\n",
      "Epoch 320/350\n",
      "91/91 - 0s - loss: 6781255.9011 - mae: 2101.2307 - val_loss: 9358109.0000 - val_mae: 2536.7473\n",
      "Epoch 321/350\n",
      "91/91 - 0s - loss: 6240766.3516 - mae: 1961.0103 - val_loss: 9569132.0000 - val_mae: 2624.8884\n",
      "Epoch 322/350\n",
      "91/91 - 0s - loss: 6243460.9231 - mae: 1985.3551 - val_loss: 9195559.0000 - val_mae: 2469.5095\n",
      "Epoch 323/350\n",
      "91/91 - 0s - loss: 6319462.3626 - mae: 2011.3872 - val_loss: 9295461.0000 - val_mae: 2526.5476\n",
      "Epoch 324/350\n",
      "91/91 - 0s - loss: 6450629.8681 - mae: 1985.7936 - val_loss: 9484092.0000 - val_mae: 2595.4397\n",
      "Epoch 325/350\n",
      "91/91 - 0s - loss: 6729996.3407 - mae: 2108.1628 - val_loss: 9206027.0000 - val_mae: 2485.1536\n",
      "Epoch 326/350\n",
      "91/91 - 0s - loss: 6201797.2418 - mae: 1998.2545 - val_loss: 9900635.0000 - val_mae: 2678.5061\n",
      "Epoch 327/350\n",
      "91/91 - 0s - loss: 6397560.7418 - mae: 2017.8054 - val_loss: 9269540.0000 - val_mae: 2449.9412\n",
      "Epoch 328/350\n",
      "91/91 - 0s - loss: 6284239.5330 - mae: 1994.7314 - val_loss: 9638822.0000 - val_mae: 2629.9268\n",
      "Epoch 329/350\n",
      "91/91 - 0s - loss: 6297197.0549 - mae: 1972.9391 - val_loss: 9158863.0000 - val_mae: 2492.1279\n",
      "Epoch 330/350\n",
      "91/91 - 0s - loss: 6178452.2253 - mae: 1971.0175 - val_loss: 9421287.0000 - val_mae: 2583.0188\n",
      "Epoch 331/350\n",
      "91/91 - 0s - loss: 6289875.0000 - mae: 1955.2852 - val_loss: 9138077.0000 - val_mae: 2482.4011\n",
      "Epoch 332/350\n",
      "91/91 - 0s - loss: 6786578.9505 - mae: 2087.8303 - val_loss: 9371921.0000 - val_mae: 2566.1768\n",
      "Epoch 333/350\n",
      "91/91 - 0s - loss: 6688436.7802 - mae: 2042.2695 - val_loss: 9158365.0000 - val_mae: 2504.5505\n",
      "Epoch 334/350\n",
      "91/91 - 0s - loss: 6813910.4725 - mae: 2114.4333 - val_loss: 9101704.0000 - val_mae: 2470.3582\n",
      "Epoch 335/350\n",
      "91/91 - 0s - loss: 7422210.9451 - mae: 2122.3000 - val_loss: 9699027.0000 - val_mae: 2669.8765\n",
      "Epoch 336/350\n",
      "91/91 - 0s - loss: 6856022.9231 - mae: 2068.5188 - val_loss: 9326079.0000 - val_mae: 2441.2424\n",
      "Epoch 337/350\n",
      "91/91 - 0s - loss: 6410518.2802 - mae: 1989.4908 - val_loss: 10011471.0000 - val_mae: 2718.5940\n",
      "Epoch 338/350\n",
      "91/91 - 0s - loss: 6291433.9615 - mae: 2009.7095 - val_loss: 9326416.0000 - val_mae: 2453.0000\n",
      "Epoch 339/350\n",
      "91/91 - 0s - loss: 7300315.5714 - mae: 2180.8938 - val_loss: 9176696.0000 - val_mae: 2512.5957\n",
      "Epoch 340/350\n",
      "91/91 - 0s - loss: 6646202.7912 - mae: 2090.4119 - val_loss: 9120069.0000 - val_mae: 2474.8496\n",
      "Epoch 341/350\n",
      "91/91 - 0s - loss: 6104175.2198 - mae: 1964.3839 - val_loss: 9801714.0000 - val_mae: 2670.0054\n",
      "Epoch 342/350\n",
      "91/91 - 0s - loss: 6153824.9835 - mae: 1936.3523 - val_loss: 9258629.0000 - val_mae: 2447.9016\n",
      "Epoch 343/350\n",
      "91/91 - 0s - loss: 6383841.7692 - mae: 2027.8251 - val_loss: 9089655.0000 - val_mae: 2507.8411\n",
      "Epoch 344/350\n",
      "91/91 - 0s - loss: 6097566.9258 - mae: 1957.1992 - val_loss: 9121103.0000 - val_mae: 2513.8582\n",
      "Epoch 345/350\n",
      "91/91 - 0s - loss: 6263377.0824 - mae: 1976.0663 - val_loss: 8999233.0000 - val_mae: 2475.6377\n",
      "Epoch 346/350\n",
      "91/91 - 0s - loss: 6447668.5659 - mae: 2051.7888 - val_loss: 9911616.0000 - val_mae: 2698.2668\n",
      "Epoch 347/350\n",
      "91/91 - 0s - loss: 6457093.9011 - mae: 1973.0844 - val_loss: 9025413.0000 - val_mae: 2495.6335\n",
      "Epoch 348/350\n",
      "91/91 - 0s - loss: 6169441.6703 - mae: 1987.0155 - val_loss: 9080888.0000 - val_mae: 2470.5315\n",
      "Epoch 349/350\n",
      "91/91 - 0s - loss: 6792078.8132 - mae: 2031.6195 - val_loss: 9101655.0000 - val_mae: 2514.4807\n",
      "Epoch 350/350\n",
      "91/91 - 0s - loss: 6582388.1593 - mae: 2062.6819 - val_loss: 9029047.0000 - val_mae: 2498.5830\n",
      "MSE: 9029047.000, RMSE: 3004.837, MAE: 2498.583\n",
      "Predicted: 17582.260\n"
     ]
    }
   ],
   "source": [
    "from numpy import sqrt\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    " \n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn asarray(X), asarray(y)\n",
    " \n",
    "# load the dataset\n",
    "path = '/tf/monthly-car-sales.csv'\n",
    "df = read_csv(path, header=0, index_col=0, squeeze=True)\n",
    "# retrieve the values\n",
    "values = df.values.astype('float32')\n",
    "# specify the window size\n",
    "n_steps = 5\n",
    "# split into samples\n",
    "X, y = split_sequence(values, n_steps)\n",
    "# reshape into [samples, timesteps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "# split into train/test\n",
    "n_test = 12\n",
    "X_train, X_test, y_train, y_test = X[:-n_test], X[-n_test:], y[:-n_test], y[-n_test:]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', kernel_initializer='he_normal', input_shape=(n_steps,1)))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=350, batch_size=32, verbose=2, validation_data=(X_test, y_test))\n",
    "# evaluate the model\n",
    "mse, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('MSE: %.3f, RMSE: %.3f, MAE: %.3f' % (mse, sqrt(mse), mae))\n",
    "# make a prediction\n",
    "row = asarray([18024.0, 16722.0, 14385.0, 21342.0, 17180.0]).reshape((1, n_steps, 1))\n",
    "yhat = model.predict(row)\n",
    "print('Predicted: %.3f' % (yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 187\n",
      "Trainable params: 187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# example of summarizing a model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(8,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broken right now, issue with import stuff + pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.4.6)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.16)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# create the dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# determine the number of input features\n",
    "n_features = X.shape[1]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "sgd = SGD(learning_rate=0.001, momentum=0.8)\n",
    "model.compile(optimizer=sgd, loss='binary_crossentropy')\n",
    "# fit the model\n",
    "history = model.fit(X, y, epochs=100, batch_size=32, verbose=0, validation_split=0.3)\n",
    "# plot learning curves\n",
    "pyplot.title('Learning Curves')\n",
    "pyplot.xlabel('Epoch')\n",
    "pyplot.ylabel('Cross Entropy')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}